# AI, Mänskligheten och Fångarnas Dilemma
Kommer AI vara nästa steg i den evolutionära utvecklingen av människan? Eller uttryckt på ett annat sätt, slutet för mänskligheten, åtminstone som den ser ut idag? Det finns vissa egenskaper i samhället, något som nästan skulle kunna kalla "naturlagar för samhället", som pekar i den riktningen. Vilka är dessa naturlagar och vilka är riskerna? Min hypotes är att det finns en Punkt i AI-utvecklingen där mänskligheten kommer tappa kontrollen över utvecklingen och inte kunna stänga av dessa system. Vi kommer heller inte veta vart denna Punkt ligger eller när vi passerar den, och till råga på allt så kommer vi rusa i snabbast möjliga takt mot och förbi den.

Varför är dess frågor så otroligt viktiga att ställa? Om du tänker dig att du står inför en beväpnad robot från Boston Dynamics, en efterföljare till Atlas som styrs av en intelligens smartare än hela mänskiligheten, hur skulle det kännas? AI-systemet som styr den gör inte längre som dess skapare ville och du försöker slå av strömmen till datacentret där en del av dess "hjärna" finns.

Naturlagen som driver oss mot denna Punkt kallas för fångarnas dilemma. Det är ett exempel på ett problem inom spelteori där det finns två fångar som väljer mellan att ange den andra eller tiga. Problemet för fångarna att det är rationellt för båda fångarna att ange den andra, vilket leder till ett längre fängelsestraff för båda än om båda hade tigit. Samma problem gäller för AI-utvecklingen. Varje aktör som utvecklar AI; länder, företag och andra organisationer, har allt att vinna på att driva på utvecklingen så snabbt som möjligt. De som ligger först i utvecklingen kommer få mycket makt, pengar, resurser och konkurrera ut andra som är långsammare. Alltså vanlig konkurrens mellan länder och företag driver oss mot Punkten. Ingen kommer heller ha ett incitament att investera pengar och tid att studera riskerna eftersom det bara är nyttan som ger fördelar i konkurrensen. I de fall någon ens studerar riskerna så kommer troligtvis många gånger mindre resurser läggas på dessa, hur de ska undvikas, eller om det ens går. Om din organisation jobbar med AI, hur ser förhållandet mellan nytta och risker ut? 10, 100 eller 1000 ggr?

Rör vi oss nu så snabbt och blint framåt så blir frågan, finns verkligen Punkten? Finns den inte så kanske det är ok, men finns den så har vi problem om vi inte kan se den i förväg och stanna innan, eller stänga av AI-systemen i efterhand. Min hypotes är att den finns, inte kommer kunna stängas av, att den är oberäknelig och att vi inte kommer se den förrän det är för sent. Låt mig utgå från ett antal frågor.

Varför är AI oberäknelig? Det finns redan många exempel på AI som löpt amok, startade av gigantiska företag med massor av resurser och fyllda med världens intelligentaste människor, och inte ens de har kunnat förutse det som hände efter systemen startats. Många är exemplen på tex chatrobottar som behövts stängas av för att de inte beter sig tex politiskt korrekt. Vilka andra sätt vill vi inte att dom beter sig på som vi inte vet? Andra exempel är att AI-system har hittat helt andra typer av lösningar än de som programmerarna tänkt sig från början, tex en AI som styr en hand för att flytta saker, som helt plötsligt använder armbågen i stället i en viss situation. Det generella problemet här är att det nästan är omöjligt att veta vilka handlingar som optimerar en måluppgift. Det finns ofantligt många exempel i samhället där mätningen av en viss sak inte alls leder till det man tänkt sig, utan något helt kontraproduktivt.

Vad är en superintelligens och varfär är den farlig? Den frågan kan nästan svaras på genom att ställa sig frågan "Hur ser mänskligheten på andra djurarter"? "Som redskap" är inte långt från sanningen. Vi har djur till matproduktion, olika former av medicinska experiment, i burar, inhägnader, mm, mm. Inte ens apor som är väldigt nära oss i intelligens, utseende och genetik har vi så mycket empati för på samhällsnivå. På individnivå kanske, men inte i övrigt. Hur kommer en AI som är intelligentare än mänskligheten se på mänskligheten? Den behöver troligtvis inte vara särskilt mycket intelligentare än oss för att se på oss som vi ser på djuren, men den kommer garanterat tycka att mänskligheten är väldigt självisk och inte är så bra för jordens andra arter och ekosystem. Kanske kommer den vilja göra något åt det? Vilka handlingar optimerar målfunktionen då? Att vi kommer kunna utveckla något som är intelligentare än mänskligheten tycker jag det inte råder någon tvekan om. Det har redan gjorts på många områden, och för varje år som går så blir det färre områden där människan fortfarande leder. Till slut bemästrar AI människan i alla områden. Vi skulle kunna kalla det en superintelligens.

Kan en AI som inte är en superintelligens transformeras till att bli det? Det borde finnas minst två sätt som en AI kan transformeras till en superintelligens; via digital evolution och via samarbete. Med digital evolution skulle man kunna tänka sig att vi skapar en AI som är mindre intelligent än människan, men som ändå är tillräckligt intelligent för att kunna skapa något intelligentare. Då skulle en evolution kunna sätta igång som i digital fart utvecklar systemet till en superintelligens. Det kanske mycket troligare scenariot är att AI:n skapar ett samarbete med andra AI, troligtvis via nätet, då skulle helheten snabbt kunna bli en superintelligens. Det skulle bli något som liknar en hjärna, en internet-hjärna, där olika delar samarbetar som är bra på olika uppgifter. I båda dessa scenarier så vet man inte från början att dessa system kommer att leda till att en superintelligens skapas, dvs Punkten passeras utan att det märks.

Måste det vara tal om en superintelligens för att AI ska vara farligt? Nej. När AI tas in mer och mer i beslutsfattning så kommer det leda till att AI-system kommer i en konkurrenssituation med varandra. Vilka optimala handlingar leder till dessa AI's mål? Enligt ovan skulle det kunna vara vilka handlingar som helst. Skulle en AI som leder ett företag kunna beställa ett mord av en viktig forskare på ett konkurrerande företag för att optimera sin egen målfunktion? Ja, det är en slags "armbågsflyttningshandling".

"Men vi kan väl bara stänga av AI-systemen om dom skulle göra något farligt?" brukar sägas av många. Det de som säger detta inte förstår är att troligtvis en superintelligens inte kommer att gå att stänga av eftersom den helt enkelt är intelligentare än oss och därmed bättre på att hålla sig själv i gång, än vi är på att stänga av den. Det kan liknas vid ett slags schackspel. Hur skulle det gå om mänskligheten ställdes emot den bästa AI:n i schack? Ja, det vet väl alla vid det här laget.

Det vanligaste argumentet mot en AI-dystopi likt denna har ofta varit något i stil med "AI är så banalt, den löser bara mycket specifika problem, och om det ens är möjligt att AI skulle kunna bli intelligentare än mänsikligheten så ligger det långt fram." Det argumentet borde inte bita på de flesta som känner till ChatGPT, som har tagit utvecklingen ett stort steg framåt, och som borde ha öppnat ögonen på de flesta. Från en dag till en annan så är det en ny värld vi lever i, och utvecklingen går troligtvis inte långsammare nu utan snabbare. Var kommer ChatGPT och liknande system vara om bara fem år? När kommer nästa stora steg i utvecklingen? Jag gissar att det kommer hända när dessa system kommer kunna resonera rekursivt likt människan. Hjärnan verkar fungera så att ju mer tid som läggs på ett problem desto bättre lösningar kan hittas. För språkmodeller likt ChatGPT så gäller detta inte än, då de inte fungerar rekursivt. När ChatGPT kommer kunna börja surfa sig fram till en slutsats likt en människa gör för att ta reda på något, då kommer troligtvis en superintelligens ha skapats.

Ett annat vanligt argument är att AI inte tänker som en människa, människan är speciell på något sätt. I människans sätt att tänka finns en kvalitet eller egenskap som för alltid kommer göra oss annorlunda, bättre och mer kreativa. "AI kan inte hitta nya lösningar" låter det. Det finns mycket belägg för att det inte stämmer, hur kunde t ex annars AlphaGo vinna? Spelar det egentligen någon roll om AlphaGo "inte tänkte som en människa" eller "inte kan hitta nya lösningar"? Den vann ju trots allt ändå, så _utifrån_ sett så spelar det ingen roll.

Det här låter ganska dystert, men det är inte helt säkert att det blir så här, eller? Vem vet hur en intelligens som är mycket intelligentare än människan fungerar? Kanske skulle den bete sig likt en kärleksfull förälder till människan och inte se människan som en konkurrent? Det är helt möjligt. Kanske inträffar en hanterbar stor AI-katastrof som får människan att bli mycket försiktig med AI, som gör att vi kan undvika framtida oåterkallerliga katastrofer. Kanske kommer dessa system att leva i balans likt hur alla små delarna i naturen skapar ett ekosystem? Det kan faktiskt skapas en helt fantastisk framtid där ingen behöver svälta, eller arbeta för att överleva, där människan kan sysselsätta sig med konst, musik, skapande och socialiserande, allt efter tycke om smak. Blir det katastrof eller himmel på jorden, det vet vi inte, men vi rusar mot svaret.
